# Deloppgave 3: Vitenskapsfilosofi


## Falsifikasjonisme

Karl Popper’s falsifiserbarhetskriterium er et kriterium som bestemmer den vitenskapelige karakteren eller statusen til en teori [@popper_conjectures_1969]. Popper mente at den fundamentale egenskapen til en vitenskapelig teori er at den er falsifiserbar, altså at man på en empirisk måte kan teste om teorien er usann. For at en teori skal være falsifiserbar mente Popper at teorien skal kunne brukes til å utlede spesifikke konsekvenser eller prediksjoner som videre kan testes ut. For å illustrere dette benyttet Popper Einsteins Relativitetsteori, ettersom Einstein predikerte at hvis teorien hans er sann vil lys bøye seg rundt store masser i universet. Denne predikasjonen er risikofylt, da enhver som klarer å måle at lyset ikke bøyer seg rundt store masser i universet vil motbevise at teorien er sann. Den har altså blitt falsifisert. Popper’s tanke var at den vitenskapelige statusen til en teori ville forsterkes av hvor risikabel den er, eller sagt på en annen måte hvor enkelt den kan falsifiseres. Det er viktig å presisere at Popper var tydelig på at en «sann» vitenskapelig teori måtte være falsifiserbar, men ikke falsifisert. Hvis en teori var falsifiserbar kunne den betraktes som en vitenskapelig teori, men skulle bli avkastet om den hadde blitt falsifisert. I praksis betyr dette at vitenskapen føres fremover av å legge frem falsifiserbare teorier for så å teste de empirisk hvor de teoriene som ikke blir falsifisert blir ivaretatt, men teoriene som blir falsifisert blir kastet. Hvis en teori blir falsifisert, men senere modifisert for å bedre passe med resultatene (dataen) ville det senke den vitenskapelige statusen til teorien. Det sistnevnte kalles «konvensjons strategi».

Bakgrunnen for falsifiserbarhetskriteriet var blant annet at Popper ønsket å kunne svare på demarkasjonsproblemet. Demarkasjonsproblemet kan enkelt forklares som spørsmålet «hva er forskjellen på vitenskap og pseudovitenskap?», pseudovitenskap er synonymt med ikke-vitenskapelig (f.eks. myter, religion osv.). Med falsifiserbarhetskriteriet mente Popper at man svarte på spørsmålet ved å strekke en rett linje mellom hva som er vitenskapelig og pseudovitenskapelig avhengig av om teorien/påstanden er falsifiserbar eller ikke, som forklart i forrige avsnitt. 

Mange vitenskapsfilosofer deler ikke Popper’s syn på demarkasjonsproblemet og falsifiserbarhetsproblemet. @okasha_philosophy_2016 argumenterer for at det blir feil å alltid skille mellom hva som er vitenskapelige teorier og pseudovitenskapelige teorier ettersom vitenskapen er heterogen i den forstand at den inneholder mange forskjellige felt (fysikk, biologi, osv) som ikke nødvendig vis er like. Okasha er altså kritisk til at Popper antar at vitenskapen har en essensiell egenskap. Videre forklarer Okasha at de egenskapene som gjør en teori vitenskapelig i et felt ikke nødvendigvis er de samme egenskapene i andre felt, dermed kan det hende at det ikke er mulig å fange hva som gjør en teori vitenskapelig i enhver situasjon med et kriterium (falsifiserbarhetskriteriet). I tillegg mener Okasha at Popper’s tanker om konvensjons strategi kanskje er for streng. Popper har kritisert enkelte forskere for å drive pseudovitenskap når de modifiserer teorier i etterkant av falsifisering, men mange store vitenskapelig oppdagelser har oppstått via nettopp denne prosessen. Okasha stiller spørsmålet om man skal definere arbeidet til de forskerne som har gjort store vitenskapelige oppdagelser for «vitenskapelig» selv om de har brukt konvensjons strategi som Popper mente svekket den vitenskapelige statusen. Denne usikkerheten mener Okasha betyr at det må finnes noe mer i vitenskap som Popper ikke har klart å fange opp. 
Jeg personlig er kanskje mer enig med Okasha enn Popper. I tillegg liker jeg ideen av å bekrefte teorier fremfor å falsifisere de.

\newpage

## HD-metoden og abduksjon/Bayesianisme

Hypotetisk deduktiv metode er, til tross for navnet, i kjernen en induktiv metode å argumentere for vitenskapelige teorier på. Det finnes flere metoder som benytter induktive argumenter, deriblant naiv induksjon, abduksjon og bayesianisme. Hypotetisk deduktiv metode ble foreslått av Hempel (Hempel, 1966) som en løsning til flere av svakhetene han mente naiv induksjon hadde. Hempel var kritisk til naiv induksjon fordi han blant annet mente at noe av det som forsterker et induktivt argument, altså at man skal ha så mange observasjoner som mulig også er en av dens svakeste egenskaper. Hempel forklarer dette med at det finnes et uendelig antall observasjoner og et uendelig antall interaksjoner mellom observasjonene, det er dermed umulig å generalisere fra all data. I tillegg presiserer Hempel at for å kunne gjøre mange undersøkelser og finne all relevant data til undersøkelsene må man begynne med å formulere et forskningsspørsmål. Hempel mener at det i praksis er en bedre måte å jobbe vitenskapelig på.

Den hypotetisk deduktive metoden bygger på to kontekster, oppdagelseskonteksten og begrunnelseskonteksten. Hempel mente at hvordan en vitenskapsmann kommer frem til eller finner på en teori, spiller ingen rolle. Man kan med andre ord gjøre en utdannet gjetning for å formulere en teori, objektiviteten til vitenskapen ivaretas igjennom de resterende trinnene/begrunnelseskonteksten. Hempel mente at det kreves dyp innsikt i eksisterende kunnskap for å kunne formulere gode teorier, og at en person som er dypt investert i et fagfelt har mye bedre forutsetninger enn en ordinær person. Etter at teorien/hypotesen er formulert skal man dedusere seg frem til hvilke empiriske konsekvenser som følger fra teorien/hypotesen. Konsekvensene må deretter testes empirisk, hvis de empiriske resultatene viser at konsekvensene er riktige/sannsynlige gir det en hvis bekreftelse til teorien. Om konsekvensene skulle være feil så blir teorien falsifisert eller delvis falsifisert. Oppdagelseskonteksten begynner ofte med et forskningsspørsmål hvor man senere kan finne opp hypoteser som tentative svar på forskningsspørsmålet, for så å sette disse opp mot empiriske tester. Et eksempel på hvordan hypotetisk deduktiv metode kan brukes kan være at jeg er interessert i spørsmålet «er aldring årsaken til at treningsresponsen typisk avtar med stigende alder?», deretter formulerer jeg en hypotese basert på utdannet gjetning «aldring hemmer trenbarheten». Neste steg blir å sette opp et studiedesign og testmetodikk som faktisk kan bekrefte hypotesen (dedusering). Videre kan jeg bekrefte/avkrefte teorien igjennom bruk av passende analysemetoder som ender med et resultat som forteller noe om sannsynligheten for at teorien er sann (induktivt argument).

Det er i midlertidig noen svakheter med hypotetisk deduktiv metode, deriblant at det ikke tar høyde for at det er flere teorier som kan forklare samme fenomen. Dette tas det ikke høyde for og det er de teoriene som forklarer dataen best som bekreftes «best», men det er ofte slik at enkle teorier gjenspeiler hva som egentlig skjer i naturen bedre enn veldig kompliserte teorier. Dette belønnes ikke av hypotetisk deduktiv metode, men abduksjon har en måte å løse dette på. Abduksjon beskrives som slutning til den beste forklaringen. Abduksjon forsøker å velge den beste teorier blant mange teorier som kan forklare samme fenomen. Teoriene bedømmes blant annet ut ifra forklaringskraft (på data) og enkelhet (antall faktorer som spiller inn). Den teorien som i sum får mest støtte av forklaringskraft og enkelthet blir preferert til å forklare fenomenet.

\newpage

## Replikasjonskrisen

Replikasjonskrisen dreier seg om at i enkelte vitenskapsfelt som f.eks biomedisin er det en stor andel etablerte effekter/teorier som viser en betydelig mindre effekt eller ingen effekt ved replikasjon av studien (Bird, 2021). Dette kan være et stort problem og har blitt foreslått til å være en av de drivene faktorene til den negative trenden for vitenskapelig tillit i samfunnet [@bird_understanding_2021-1]. Bird foreslår at grunnen til at replikasjonskrisen har oppstått er det som kalles basefrekvensfeil. Bird forklarer basefrekvensfeil som at det er et misforhold mellom bevis for hendelsen og den faktiske sannsynligheten for hendelsen (basefrekvens). Bird forklarer videre at i enkelte forskningsfelt er det vanskelig å finne opp en hypotese som er sann, dette medfører at de fleste hypotesene som testes er usanne. Som et eksempel illustrere han at om 90% av alle hypotesene som testes er usanne og grensen for signifikans er satt til < 5% vil 32% av alle positive resultater være falsk positiv (type 1 feil). Type 1 feil betyr at man konkluderer med at en teori er sann når den egentlig er usann. Dette medfører selvfølgelig at når disse falske positive resultatene blir forsøkt replisert, ender 95% opp som «riktig» negative. Dette fører til replikasjonskrisen. Videre gir Bird tre mulige løsninger til problemet. (1) aksepter at det er slik forskningen må foregå, men juster kredibiliteten i publikasjonene deretter. (2) Finn en løsning til hvordan man kan komme opp med en større andel sanne hypoteser. (3) Øk standarden på forskningen ved å senke signifikansnivået lavere enn 5%. 

Det finnes andre teorier for hvordan replikasjonskrisen har oppstått, deriblant tvilsom forskningspraksis, for lav statistisk styrke og publikasjonsskjevhet. Tvilsom forskningspraksis dreier seg om dårlig kvalitet på eksperimenter eller fabrikkering av data osv. Bird mener det er vanskelig å måle dårlig forskningspraksis og at det til tross for veldig god kvalitet på forskningen vil en høy signifikansgrense fortsatt gi mange falske positive resultater. For lav statistisk styrke betyr at studier har en dårlig egenskap til å oppfatte fenomener som faktisk er der, det kan skyldes mange ting deriblant et lavt antall forsøkspersoner. Bird argumenterer for at den statistiske styrken ikke har noen særlig stor påvirkningskraft hvis basefrekvensfeilen er betydelig til stede. Publikasjonsskjevhet er all skjevhet hvor ulike resultater påvirker publikasjonsraten, det kan både være at journaler ønsker å publisere positive resultater, men også at forskeren ikke ønsker å publisere negative resultater. Bird er enig i at dette kan være en av faktorene til at replikasjonskrisen eksiterer, men argumenterer for at det ikke kan være den viktigste grunnen. Dette fordi resultatene som blir publisert kan ha vært av høy standard og med en lav signifikansgrense som gir få falske positive resultater. 

Jeg er ganske enig med Bird sitt resonnement om replikasjonskrisen og at det i hovedsak skyldes basefrekvensfeil. I forskningsfelt som treningsfysiologi tror jeg at for lav statistisk styrke også er av stor betydning. Dette ettersom en stor andel forskningsstudier innenfor dette fagfeltet har et lavt antall forsøkspersoner og mindre reliable testprotokoller på grunn av biologisk variasjon som i sum gir en lav statistisk styrke. 

